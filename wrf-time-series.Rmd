---
title: "Assessment_2"
author: "Percy Ekanem"
date: '2023-04-08'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
wrfdata <- read.csv('WRFdata_May2018.csv')

#Importing libraries
library(tidyverse)
library(dplyr)
library(sqldf)
library(zoo)
library(ggplot2)
library(osmdata)
library(outliers)
library(moments)
library(DescTools)
library(tseries)
library(forecast)
library(randomForest)
library(lubridate)
library(corrplot)
library(e1071)
```

```{r}
# a <- c("a", "b", "a","c","a","b","d","a","b","c")
# paste(a, ave(a,a, FUN = seq_along), sep = ".")
# rep(a, length.out = ncol(wrfdata))
# seq_along(a)
```

```{r}
rename_cols <- function(data){
  name <- as.character(data[1,1:ncol(data)])
  name <- gsub('"',"", name)
  name <- unique(name)[c(-1, -2, -13)]
  name <- rep(name, length.out = (ncol(data)-2))
  SN <- ave(name, name, FUN = seq_along)
  name <- paste(name, SN, sep = ".")
  name <- append(name, "XLAT", after = 0)
  name <- append(name, "XLONG", after = 1)
  colnames(data) <- name
  data <- data[-1,]
  return(data)
}

```

```{r}
newdata <- rename_cols(wrfdata)
str(newdata)
```

```{r}
newdata <- newdata %>% 
  filter(complete.cases(XLAT) & complete.cases(XLONG)) %>%
  mutate(coordinates = paste(XLAT, ",", XLONG))

which(names(newdata) == "coordinates") #finding the index of coordinates (2483)
ncol(newdata) #finding the total number of columns

newdata <- newdata %>% select(1, 2, 2483, everything())

length(unique(newdata$coordinates)) == length(newdata$coordinates) #There are no duplicate coordinate values



# write.csv(newdata, "output1.csv", row.names = FALSE)

```

```{r}
#After exporting, it was discovered that row 25 of V10.1 was a string with the value "[]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]"
newdata$V10.1[25]
#Converting the data to numeric in the reorder_data function returns NA for this
#row (tested in a chunk right after defining the function)

```

```{r}
# #To understand what gather and spread do.
# newdata[1:10,1:2] %>%
#   gather(key = "variables", value = "values") %>%
#   spread(key = "values", value = "variables")
```

```{r}
getbb("Derry City") #From the osmdata library

#Derry Longitude: -7.161306 to -7.481306, Latitude: 54.837868 to 55.157868
#Approximate center coordinates according to Google: 54.9966, -7.3086

#Viewing all three locations within that range.
newdata %>% 
  filter(as.numeric(XLONG) >= -7.481306 & as.numeric(XLONG) <= -7.161306) %>% 
  filter(as.numeric(XLAT) >= 54.837868 & as.numeric(XLAT) <= 55.157868)

#Closest to center is 55.011 , -7.364, which is on row 1526

which(newdata$coordinates == "55.011 , -7.364")

#The other  two are located on rows 1525 and 1527
which(newdata$XLONG >= -7.161306 & newdata$XLONG <= -7.481306 & 
        newdata$XLAT >= 54.837868 & newdata$XLAT <= 55.157868)

# 
# #Old bounding box. 
# newdata %>% 
#   filter(XLONG >= -6.5947 & XLONG <= -7.4117) %>% 
#   filter(XLAT >= 54.9753 & XLAT <= 55.0285)
# 
# which(newdata$XLONG >= -6.5947 & newdata$XLONG <= -7.4117 
#       & newdata$XLAT >= 54.9753 & newdata$XLAT <= 55.0285)




which(newdata$coordinates == "55.011 , -7.364")

#Rows 1500 to 1850 will be used for analysis.

```

```{r}
reorder_data <- function(data){
  data <- data %>% mutate(row_id = row_number())
  
  identifiers <- data %>% select(c(XLAT, XLONG, coordinates, row_id))
  
  ordered <- data %>%
    select(-c(XLAT, XLONG, coordinates)) %>%
    gather(key = "column_name", value = "value", -row_id) %>%
    separate(col = "column_name", into = c("column_name", "SN"), sep = "\\.") %>%
    mutate(SN = as.numeric(SN)) %>%
    arrange(row_id, SN) %>% # sort by row ID and SN
    spread(key = "column_name", value = "value")
  
  identified <- sqldf("SELECT a.coordinates, b.* FROM identifiers a JOIN ordered b ON
                      a.row_id = b.row_id")
  identified <- identified %>% select(-row_id)
  final <- as.data.frame(lapply(identified %>% 
                                select(-coordinates), as.numeric)) %>% 
    mutate(coordinates = identified$coordinates) %>%
    select(coordinates,everything())
  
  return(final)
}
```

```{r}
# #
# testing_for_V10 <- reorder_data(newdata[25,])
# testing_for_V10$V10[1]
```

```{r}
dates <- names(wrfdata)

sum(is.na(dates))

#viewed in excel, showed that the last date and time entry (31.05.2018.21.00) was replaced with X.2225

dates <- gsub("X.2225","X31.05.2018.21.00", dates)
dates <- gsub("X","", dates)

dates <- dates[nchar(dates) > 6]
length(dates)

#view(datetime)
```

```{r}
#creating a function to add and format the date & time, reorder the columns to 
#match the original dataset, and delete the "SN"(serial number) column
add_date <- function(data, datetime){
  numbers <- unique(data$SN)
  
  datetime_df <- data.frame(datetime, numbers)
  
  data <- sqldf("select data.*, datetime_df.datetime 
                       from data 
                       join datetime_df 
                       on (data.SN=datetime_df.numbers)")
  
  column_order <- c("datetime", "coordinates", "SN", "TSK", "PSFC", "U10", 
                    "V10", "Q2", "RAINC", "RAINNC", "SNOW", "TSLB", "SMOIS")
  
  ordered_columns <- data[column_order]
  ordered_columns <- ordered_columns %>% select(-SN)
  
  output <- ordered_columns %>%
    mutate(datetime = format(strptime(datetime, format="%d.%m.%Y.%H.%M"),"%d-%m-%Y %H:%M")) %>%
     mutate(datetime = as.POSIXct(datetime, format = "%d-%m-%Y %H:%M")) %>%
    mutate(day = as.numeric(mday(datetime)), hour = as.numeric(hour(datetime))) %>%
    select(datetime, day, hour, everything())

  return(output)
}

```


```{r}
# #Testing both functions on first row of dataset
# add_date(reorder_data(newdata[1,]), dates)
# 
# #Nice. B-)


data_sample <- add_date(reorder_data(newdata[1500:1850,]), dates)

# write.csv(data_sample, "output2.csv", row.names = FALSE)
```

```{r}
# Checking that the last column of data sample matches the corresponding column in newdata

as.numeric(newdata[1850,2474:2483]) == as.numeric(data_sample[nrow(data_sample),5:14])
#as.numeric() was added because the "snow" column in newdata had no decimal, and
#therefore returned FALSE

length(which(is.na(data_sample$RAINC) & !is.na(data_sample$RAINNC)))



#Wind speed is the square root of (U^2 +V^2)
data_sample <- data_sample %>% 
  mutate(wind_speed = sqrt(U10^2 + V10^2)) %>%
  select(1:6, wind_speed, everything(), -U10, -V10)

#Precipitation is the sum of convectional and non convectional rain
data_sample <- data_sample %>% 
  mutate(precipitation = RAINC + RAINNC) %>%
  select(1:8, precipitation, everything(), -RAINC, -RAINNC)

#Converting surface temperature from Kelvin to Celcius
data_sample <- data_sample %>% 
  mutate(surface_temperature = TSK- 273.15) %>%
  select(1:4, surface_temperature, everything(), -TSK)
#Converting soil temperature from Kelvin to Celcius
data_sample <- data_sample %>% 
  mutate(soil_temperature = TSLB- 273.15) %>%
  select(1:10, soil_temperature, everything(), -TSLB)

#Converting Q2 to grams.
#Q2 (specific humidity) is recorded as kg of water vapor per kg of air (kg/kg)
#https://www.weather.gov/lmk/humidity specific humidity is usually g/kg
data_sample <- data_sample %>%
  mutate(humidity_2m = Q2*1000) %>%
  select(1:7, humidity_2m, everything(), -Q2)

#Converting PSFC to KPa
data_sample <- data_sample %>% 
  mutate(surface_pressure = PSFC/1000) %>%
  select(1:5, surface_pressure, everything(), -PSFC)

#Converting SMOIS to percentage
data_sample <- data_sample %>% 
  mutate(soil_moisture = SMOIS*100) %>%
  select(everything(), soil_moisture, -SMOIS)

data_sample <- data_sample %>% select(-SNOW)

summary(data_sample)
```


# Exploratory Data Analysis

```{r}
# Different parameters that can be used for 
# 
# TSC (skin/surface temperature) can be used to measure potential of solar energy
# precipitation can be used to measure potential of hydro energy. 
# wind speed can be used to measure potential of wind energy
# 
# Q2 (humidity) can also affect solar and wind energy (read articles & papers).
# Precipitation is too low in target data so hydro energy does not seem viable.
# 
#https://www-sciencedirect-com.ezproxy.bolton.ac.uk/science/article/pii/S0264837718316429#sec0025
#Use table in section 3.3 for descriptive statistics

```

```{r}
summary(data_sample)
```

```{r}
target_data <- data_sample %>% filter(coordinates == "55.011 , -7.364")
summary(target_data)
```

## Outlier Handling

```{r}
#Order of outlier handling

#Grubb's -> boxplot -> histogram -> QQPlot -> z-score -> Skewness & kurtosis -> transformation or winsorization.
#z score shows the number of outliers, skewness & kurtosis show the effects.
# (boxplots are based on iqr)
#Grubb's test assumes that the data is normally distributed and tests the null hypothesis that there are no outliers.
#Histogram provides a visual representation and will show approximately whether the data is normally distributed.
#Shapiro-Wilk test formally tests for normality of the distribution.
#QQPlot also tests for normality.
#z-score provides a different way of detecting the presence of outliers (different from IQR)
#Transformation converts the data to the logarithmic scale.
#Winsorisation replaces outliers with values on the percentile closest to them (low outliers with 5th percentile, high outliers with the 95th percentile, usually).
#Winsorisation keeps the current scale while reducing the impact of outliers.
```

```{r}
# #Grubbs test experiment
# grubbs.test(c(1,2,3,2,3,1,1,9), type = 10)
# #p = 0.001227 (less than 0.05) and 9 is obviously an outlier, so
# 
# grubbs.test(c(4, 6, 7,5,7,4,5,6,10), type = 10, opposite = TRUE)
# #opposite makes it check the lowest value.
# #p = 1, lowest value is not an outlier.
# 
# grubbs.test(c(0.3, 4, 6, 7,5,7,4,5,6,10), type = 11)
# #type 11 checks highest & lowest. p <0.05, both are outliers
# 
# grubbs.test(c(4, 6, 7,5,7,4,5,6,10), type = 11)
# #p>0.05 even though one of them is an outlier.
# 
# grubbs.test(c(4, 6, 7,5,7,4,5,6, 0.3), type = 11)
# #p>0.05 even though one is an outlier.
```

### Surface Temperature

```{r}
#Grubb's test (from the outliers library)
#Null hypothesis is that the highest or lowest value is not an outlier.
#p < 0.05 means to reject the null hypothesis.
grubbs.test(data_sample$surface_temperature, type = 10, opposite = FALSE)
grubbs.test(data_sample$surface_temperature, type = 10, opposite = TRUE)
#p>0.05 for both upper and lower, outliers unlikely.

grubbs.test(target_data$surface_temperature, type = 10, opposite = FALSE)
grubbs.test(target_data$surface_temperature, type = 10, opposite = TRUE)
#p>0.05 for both upper and lower, outliers unlikely.
```

```{r}
#Boxplot
boxplot(data_sample$surface_temperature, target_data$surface_temperature,
        main = "Boxplot to Determine Outliers",
        names = c("sample","target"),
        xlab = "Surface Temperature")

#showed outliers in sample but not target. grubb's test assumes normality, and 
#is also more effective in smaller sample sizes.

#Histogram
hist(data_sample$surface_temperature,
     main = "Surface Temperature of data sample")
hist(target_data$surface_temperature,
     main = "Surface Temperature of target data")


##Quantile-quantile plot
#To compare the distribution 
qqnorm(data_sample$surface_temperature, main = "Surface Temperature of data sample")
qqline(data_sample$surface_temperature)
#significantly deviates from normal distribution

qqnorm(target_data$surface_temperature, main = "Surface Temperature of target data")
qqline(target_data$surface_temperature)
#Does not significantly deviate from normal distribution
```

```{r}
#z-score
z_score_outliers <- function(data, threshold){
  z_score <- scale(data)
  n_outliers <- length(data[abs(z_score) > threshold])
  n_data <- length(data)
  outlier_percentage <- (n_outliers/n_data)*100
  paste(round(outlier_percentage, 2), "% of the data are outliers.")
}
```

```{r}
z_score_outliers(data_sample$surface_temperature, 2)
z_score_outliers(target_data$surface_temperature, 2)
```

```{r}
#Skewness & Kurtosis (in the moments library)
#If data are skewed, median may be more appropriate than mean for use
#If data has high or low kurtosis, it is more likely to have outliers

# For the purpose of this analysis, a range of -1 to +1 and -3 to +3 will be considered an acceptable range for  skewness and kurtosis respectively. Skewness values between -1 and +1 indicate that the data is roughly symmetric, while kurtosis values between -3 and +3 indicate that the data has a moderate level of peakedness and tail thickness.

skewness(data_sample$surface_temperature, na.rm = TRUE) #data is skewed to the 
#right. skewness is 1.051077 (>1), meaning that the data is relatively asymmetrical.
kurtosis(data_sample$surface_temperature, na.rm = TRUE) #data is extremely
#leptokutic. kurtosis = 6.336047 and indicates a veery high peak.
#outliers have a high impact on the data and will be scaled using log transformation.


skewness(target_data$surface_temperature, na.rm = TRUE) #data is skewed to the 
#right (value is >0 and positive) 0.2837271 is a relatively small skewness, 
#though.
kurtosis(target_data$surface_temperature, na.rm = TRUE) #data is moderately 
#leptokurtic. kurtosis = 2.280124 and indicates a high, narrow peak.
#outliers do not seem to have a high impact on the data.

```


```{r}
# #Using log transformation to deal with outliers
# test_data <- data_sample %>% mutate(surface_temperature= log(surface_temperature))
# z_score_outliers(test_data$surface_temperature, 2) #The percentage of outliers reduced from 10.93% to 7.9%
# skewness(test_data$surface_temperature, na.rm = TRUE)
# kurtosis(test_data$surface_temperature, na.rm = TRUE)#skewness and kurtosis both increased exponentially
# hist(test_data$surface_temperature) #as shown in the histogram
# boxplot(test_data$surface_temperature) #the data also appears much more skewed, and non-outlier values seem to have a much narrower range
# 
# #This may be because there is data from multiple locations.
# 
# test_data_2 <- data_sample %>% 
#   mutate(surface_temperature = Winsorize(surface_temperature, probs = c(0.05, 0.9), na.rm = TRUE))
# z_score_outliers(test_data_2$surface_temperature, 2) #No significant change in outlier %
# skewness(test_data_2$surface_temperature, na.rm = TRUE) 
# kurtosis(test_data_2$surface_temperature, na.rm = TRUE) #skewness and kurtosis both reduced to acceptable levels
# hist(test_data_2$surface_temperature) #as shown in the histogram
# boxplot(test_data_2$surface_temperature) #the data also appears much more skewed, and non-outlier values seem to have a much narrower range
```

### Surface Pressure

```{r}
#Grubb's test (from the outliers library)
#Null hypothesis is that the highest or lowest value is not an outlier.
#p < 0.05 means to reject the null hypothesis.
grubbs.test(data_sample$surface_pressure, type = 10, opposite = FALSE)
grubbs.test(data_sample$surface_pressure, type = 10, opposite = TRUE)
#p<0.05 for lower, outliers likely. 
#p>0.05 for higher, outliers unlikely

grubbs.test(target_data$surface_pressure, type = 10, opposite = FALSE)
grubbs.test(target_data$surface_pressure, type = 10, opposite = TRUE)
#p>0.05 for both upper and lower, outliers unlikely.
```

```{r}
#Boxplot
boxplot(data_sample$surface_pressure, target_data$surface_pressure,
        main = "Boxplot to Determine Outliers",
        names = c("sample","target"),
        xlab = "Surface Pressure")

#showed outliers in sample but not target. corresponds with grubb's test.

#Histogram
hist(data_sample$surface_pressure,
     main = "Surface Pressure of data sample")
hist(target_data$surface_pressure,
     main = "Surface Pressure of target data")
#Both appear skewed to the left.


##Quantile-quantile plot
#To compare the distribution 
qqnorm(data_sample$surface_pressure, main = "Surface Pressure of data sample")
qqline(data_sample$surface_pressure)
#does not significantly deviate from normal distribution

qqnorm(target_data$surface_pressure, main = "Surface Pressure of target data")
qqline(target_data$surface_pressure)
#Does not significantly deviate from normal distribution
```



```{r}
z_score_outliers(data_sample$surface_pressure, 2)
z_score_outliers(target_data$surface_pressure, 2)
```


```{r}
#Skewness & Kurtosis (in the moments library)
#If data are skewed, median may be more appropriate than mean for use
#If data has high or low kurtosis, it is more likely to have outliers

# For the purpose of this analysis, a range of -1 to +1 and -3 to +3 will be considered an acceptable range for  skewness and kurtosis respectively. Skewness values between -1 and +1 indicate that the data is roughly symmetric, while kurtosis values between -3 and +3 indicate that the data has a moderate level of peakedness and tail thickness.

skewness(data_sample$surface_pressure, na.rm = TRUE) #data is slightly skewed to the 
#left. skewness is -0.7606164 (<1), not significantly asymmetrical.
kurtosis(data_sample$surface_pressure, na.rm = TRUE) #data is mesokurtic (normal)
#kurtosis = 0.4941651.
#outliers have no impact on the data.


skewness(target_data$surface_pressure, na.rm = TRUE) #data is skewed to the 
#left (value is >0 and negative) -0.6288644 is a relatively symmetrical.
kurtosis(target_data$surface_pressure, na.rm = TRUE) #data is mesokurtic (normal) 
#kurtosis = -0.01892303
#outliers do not seem to have any impact on the data.

```

```{r}
#Winsorization
target_data <- target_data %>% 
  mutate(surface_pressure = Winsorize(surface_pressure, probs = c(0.05, 0.95), na.rm = TRUE))

boxplot(target_data$surface_pressure, main ="Surface Pressure of target data (Winsorised)")
```

### Wind speed

```{r}
#Grubb's test (from the outliers library)
#Null hypothesis is that the highest or lowest value is not an outlier.
#p < 0.05 means to reject the null hypothesis.
grubbs.test(data_sample$wind_speed, type = 10, opposite = FALSE)
grubbs.test(data_sample$wind_speed, type = 10, opposite = TRUE)
#p>0.05 for both upper and lower, outliers unlikely.

grubbs.test(target_data$wind_speed, type = 10, opposite = FALSE)
grubbs.test(target_data$wind_speed, type = 10, opposite = TRUE)
#p>0.05 for both upper and lower, outliers unlikely.
```

```{r}
#Boxplot
boxplot(data_sample$wind_speed, target_data$wind_speed,
        main = "Boxplot to Determine Outliers",
        names = c("sample","target"),
        xlab = "Wind Speed")

#showed outliers in upper ranges of both sample and target. grubb's test assumes
#normality, and is also more effective in smaller sample sizes.

#Histogram
hist(data_sample$wind_speed,
     main = "Wind Speed of data sample")
hist(target_data$wind_speed,
     main = "Wind Speed of target data")
#both sample and target appear skewed to the right, more pronounced in sample.


##Quantile-quantile plot
#To compare the distribution 
qqnorm(data_sample$wind_speed, main = "Wind Speed of data sample")
qqline(data_sample$wind_speed)
#does not deviate significantly from normal distribution

qqnorm(target_data$wind_speed, main = "Wind Speed of target data")
qqline(target_data$wind_speed)
#Does not significantly deviate from normal distribution
```

```{r}
z_score_outliers(data_sample$wind_speed, 2)
z_score_outliers(target_data$wind_speed, 2)
```

```{r}
#Skewness & Kurtosis (in the moments library)
#If data are skewed, median may be more appropriate than mean for use
#If data has high or low kurtosis, it is more likely to have outliers

# For the purpose of this analysis, a range of -1 to +1 and -3 to +3 will be considered an acceptable range for  skewness and kurtosis respectively. Skewness values between -1 and +1 indicate that the data is roughly symmetric, while kurtosis values between -3 and +3 indicate that the data has a moderate level of peakedness and tail thickness.

skewness(data_sample$wind_speed, na.rm = TRUE) #data is skewed to the 
#right. skewness is 0.8568115 (<1), meaning that the data is symmetrical.
kurtosis(data_sample$wind_speed, na.rm = TRUE) #data is moderately
#leptokutic. kurtosis = 3.549525 and indicates a high peak.



skewness(target_data$wind_speed, na.rm = TRUE) #data is skewed to the 
#right (value is >0 and positive) 0.6044626 is a relatively small skewness, 
#though.
kurtosis(target_data$wind_speed, na.rm = TRUE) #data is moderately 
#leptokurtic. kurtosis = 3.190016 (>2) and indicates a high, narrow peak.


```

```{r}
target_data <- target_data %>% 
  mutate(wind_speed = Winsorize(wind_speed, probs = c(0.05, 0.9), na.rm = TRUE))

boxplot(target_data$wind_speed, main = "Wind speed of target data (Winsorised)")
```


```{r}
summary(data_sample$wind_speed)
#outliers have a moderate impact on the data and will be scaled.

summary(target_data$wind_speed)
#outliers do not seem to have a high impact on the data.
```

```{r}
#Using winsorization to deal with outliers
# data_sample <- data_sample %>% 
#   mutate(wind_speed = Winsorize(wind_speed, probs = c(0.05, 0.9), na.rm = TRUE))
# z_score_outliers(data_sample$wind_speed, 2) #outliers reduced from 10.19% to 5.7%
# skewness(data_sample$wind_speed, na.rm = TRUE) 
# kurtosis(data_sample$wind_speed, na.rm = TRUE) #skewness and kurtosis both reduced to acceptable levels
# hist(data_sample$wind_speed)
# boxplot(data_sample$wind_speed)
```

### Humidity at 2m

```{r}
#Grubb's test (from the outliers library)
#Null hypothesis is that the highest or lowest value is not an outlier.
#p < 0.05 means to reject the null hypothesis.
grubbs.test(data_sample$humidity_2m, type = 10, opposite = FALSE)
grubbs.test(data_sample$humidity_2m, type = 10, opposite = TRUE)
#p>0.05 for both upper and lower, outliers unlikely.

grubbs.test(target_data$humidity_2m, type = 10, opposite = FALSE)
grubbs.test(target_data$humidity_2m, type = 10, opposite = TRUE)
#p>0.05 for both upper and lower, outliers unlikely.
```

```{r}
#Boxplot
boxplot(data_sample$humidity_2m, target_data$humidity_2m,
        main = "Boxplot to Determine Outliers",
        names = c("sample","target"),
        xlab = "Humidity")

#showed outliers in sample but not in target. grubb's test assumes
#normality, and is also more effective in smaller sample sizes.

#Histogram
hist(data_sample$humidity_2m,
     main = "Humidity of data sample")
hist(target_data$humidity_2m,
     main = "Humidity of target data")
#sample appears to be a normal distribution, target does not. may need to be scaled.


##Quantile-quantile plot
#To compare the distribution 
qqnorm(data_sample$humidity_2m, main = "Humidity of data sample")
qqline(data_sample$humidity_2m)
#does not deviate significantly from normal distribution

qqnorm(target_data$humidity_2m, main = "Humidity of target data")
qqline(target_data$humidity_2m)
#Does not significantly deviate from normal distribution
```

```{r}
z_score_outliers(data_sample$humidity_2m, 2)
z_score_outliers(target_data$humidity_2m, 2)
```

```{r}
#Skewness & Kurtosis (in the moments library)
#If data are skewed, median may be more appropriate than mean for use
#If data has high or low kurtosis, it is more likely to have outliers

# For the purpose of this analysis, a range of -1 to +1 and -3 to +3 will be considered an acceptable range for  skewness and kurtosis respectively. Skewness values between -1 and +1 indicate that the data is roughly symmetric, while kurtosis values between -3 and +3 indicate that the data has a moderate level of peakedness and tail thickness.

skewness(data_sample$humidity_2m, na.rm = TRUE) #data is skewed to the 
#right. skewness is 0.1405808 (>1), meaning that the data is symmetrical.
kurtosis(data_sample$humidity_2m, na.rm = TRUE) #data is moderately
#leptokutic. kurtosis = 2.830688 and indicates a high peak.
#outliers have a moderate impact on the data and will be scaled.


skewness(target_data$humidity_2m, na.rm = TRUE) #data is skewed to the 
#right (value is >0 and positive) 0.3627937 is a relatively small skewness, 
#though.
kurtosis(target_data$humidity_2m, na.rm = TRUE) #data is moderately 
#leptokurtic. kurtosis = 2.711326 and indicates a high, narrow peak.
#outliers do not seem to have a high impact on the data.

```

```{r}
summary(data_sample$humidity_2m)
summary(target_data$humidity_2m)
```

```{r}
#Using winsorization to deal with outliers
# data_sample <- data_sample %>% 
#   mutate(humidity_2m = Winsorize(humidity_2m, probs = c(0.05, 0.9), na.rm = TRUE))
# z_score_outliers(data_sample$humidity_2m, 2) #outliers reduced from 6.75% to 2.87%
# skewness(data_sample$humidity_2m, na.rm = TRUE) 
# kurtosis(data_sample$humidity_2m, na.rm = TRUE) #skewness and kurtosis both reduced to acceptable levels
# hist(data_sample$humidity_2m)
# boxplot(data_sample$humidity_2m)
# 
# 
# target_data <- target_data %>% 
#   mutate(humidity_2m = Winsorize(humidity_2m, probs = c(0.05, 0.9), na.rm = TRUE))
# z_score_outliers(target_data$humidity_2m, 2) #outliers reduced from 6.45% to 3.23%
# skewness(target_data$humidity_2m, na.rm = TRUE) 
# kurtosis(target_data$humidity_2m, na.rm = TRUE) #skewness and kurtosis both reduced to acceptable levels
# hist(target_data$humidity_2m)
# boxplot(target_data$humidity_2m)
```

### Precipitation

```{r}
#Grubb's test (from the outliers library)
#Null hypothesis is that the highest or lowest value is not an outlier.
#p < 0.05 means to reject the null hypothesis.
grubbs.test(data_sample$precipitation, type = 10, opposite = FALSE)
grubbs.test(data_sample$precipitation, type = 10, opposite = TRUE)
#p>0.05 for lower, outliers unlikely.
#p < 0.05 for upper. Outliers likely.

grubbs.test(target_data$precipitation, type = 10, opposite = FALSE)
grubbs.test(target_data$precipitation, type = 10, opposite = TRUE)
#p>0.05 for lower, outliers unlikely.
#p < 0.05 for upper. Outliers likely.
```

```{r}
#Boxplot
boxplot(data_sample$precipitation, target_data$precipitation,
        main = "Boxplot to Determine Outliers",
        names = c("sample","target"),
        xlab = "Precipitation")

#both sample and target showed extreme outliers in upper range.

#Histogram
hist(data_sample$precipitation,
     main = "Precipitation of data sample")
hist(target_data$precipitation,
     main = "Precipitation of target data")
#both are abnormal and seem to be extremely skewed to the right.


##Quantile-quantile plot
#To compare the distribution 
qqnorm(data_sample$precipitation, main = "Precipitation of data sample")
qqline(data_sample$precipitation)
#deviates significantly from normal distribution

qqnorm(target_data$precipitation, main = "Precipitation of target data")
qqline(target_data$precipitation)
#Deviates significantly from normal distribution
```

```{r}
z_score_outliers(data_sample$precipitation, 2)
z_score_outliers(target_data$precipitation, 2)
```

```{r}
#Skewness & Kurtosis (in the moments library)
#If data are skewed, median may be more appropriate than mean for use
#If data has high or low kurtosis, it is more likely to have outliers

# For the purpose of this analysis, a range of -1 to +1 and -3 to +3 will be considered an acceptable range for  skewness and kurtosis respectively. Skewness values between -1 and +1 indicate that the data is roughly symmetric, while kurtosis values between -3 and +3 indicate that the data has a moderate level of peakedness and tail thickness.

skewness(data_sample$precipitation, na.rm = TRUE) #data is extremely skewed to the right,
#as seen in histogram. skewness is 5.542437.
kurtosis(data_sample$precipitation, na.rm = TRUE) #data is extremely
#leptokutic. kurtosis = 41.49779 and indicates a high peak.
#outliers have an extreme impact on the data and will be scaled.


skewness(target_data$precipitation, na.rm = TRUE) #data is skewed to the 
#right (value is >0 and positive) 4.463798 is a very large skewness.

kurtosis(target_data$precipitation, na.rm = TRUE) #data is extremely 
#leptokurtic. kurtosis = 24.90137 and indicates a high, narrow peak.
#outliers seem to have a high impact on the data and will be scaled.
```

```{r}
summary(data_sample$precipitation)
summary(target_data$precipitation)
#Scaling outliers using log transformation is necessary.
```

```{r}
#Using log transformation to deal with outliers
#Since there are so many zero values in the data, a constant of 0.1 will be added to make the transformation more effective.
# 
# data_sample <- data_sample %>% 
#   mutate(precipitation = precipitation + 0.1) %>%
#   mutate(precipitation = log(precipitation))
# z_score_outliers(data_sample$precipitation, 2) #outliers increased from 9.44% to 12.2%
# skewness(data_sample$precipitation, na.rm = TRUE) 
# kurtosis(data_sample$precipitation, na.rm = TRUE) #skewness and kurtosis both reduced significantly.
# hist(data_sample$precipitation)
# boxplot(data_sample$precipitation) #outliers visually reduced and range of non-outlier values increased.


target_data <- target_data %>% 
  mutate(precipitation = precipitation + 0.1) %>%
  mutate(precipitation = log(precipitation))
z_score_outliers(target_data$precipitation, 2) #outliers increased from 6.85% to 10.08%
skewness(target_data$precipitation, na.rm = TRUE) 
kurtosis(target_data$precipitation, na.rm = TRUE) #skewness and kurtosis both reduced significantly
hist(target_data$precipitation)
boxplot(target_data$precipitation) #outliers visually reduced and range of non-outlier values increased.
```

### Soil Temperature

```{r}
#Grubb's test (from the outliers library)
#Null hypothesis is that the highest or lowest value is not an outlier.
#p < 0.05 means to reject the null hypothesis.
grubbs.test(data_sample$soil_temperature, type = 10, opposite = FALSE)
grubbs.test(data_sample$soil_temperature, type = 10, opposite = TRUE)
#p>0.05 for both higher and lower, outliers unlikely. 

grubbs.test(target_data$soil_temperature, type = 10, opposite = FALSE)
grubbs.test(target_data$soil_temperature, type = 10, opposite = TRUE)
#p>0.05 for both upper and lower, outliers unlikely.
```

```{r}
#Boxplot
boxplot(data_sample$soil_temperature, target_data$soil_temperature,
        main = "Boxplot to Determine Outliers",
        names = c("sample","target"),
        xlab = "Soil Temperature")

#showed outliers in sample but not target. grubb's test assumes normality.

#Histogram
hist(data_sample$soil_temperature,
     main = "Soil Temperature of data sample")
hist(target_data$soil_temperature,
     main = "Soil Temperature of target data")
#Sample is extremely skewed to the right, while target is slightly skewed.


##Quantile-quantile plot
#To compare the distribution 
qqnorm(data_sample$soil_temperature, main = "Soil Temperature of data sample")
qqline(data_sample$soil_temperature)
#significantly deviates from normal distribution

qqnorm(target_data$soil_temperature, main = "Soil Temperature of target data")
qqline(target_data$soil_temperature)
#Does not significantly deviate from normal distribution
```



```{r}
z_score_outliers(data_sample$soil_temperature, 2)
z_score_outliers(target_data$soil_temperature, 2)
```

```{r}
#Skewness & Kurtosis (in the moments library)
#If data are skewed, median may be more appropriate than mean for use
#If data has high or low kurtosis, it is more likely to have outliers

# For the purpose of this analysis, a range of -1 to +1 and -3 to +3 will be considered an acceptable range for  skewness and kurtosis respectively. Skewness values between -1 and +1 indicate that the data is roughly symmetric, while kurtosis values between -3 and +3 indicate that the data has a moderate level of peakedness and tail thickness.

skewness(data_sample$soil_temperature, na.rm = TRUE) #data is slightly skewed to the 
#right. despite the histogram, skewness is 1.207191 (>1), meaning the data is not significantly asymmetrical.
kurtosis(data_sample$soil_temperature, na.rm = TRUE) #data is mesokurtic (normal)
#kurtosis = 0.09541563.
#outliers have no impact on the data.


skewness(target_data$soil_temperature, na.rm = TRUE) #data is skewed to the 
#right (value is >0 and positive) 0.3118738 is a relatively symmetrical.
kurtosis(target_data$soil_temperature, na.rm = TRUE) #data is mesokurtic (normal) 
#kurtosis = -0.525664
#outliers do not seem to have any impact on the data.

```

### Soil Moisture

```{r}
#Grubb's test (from the outliers library)
#Null hypothesis is that the highest or lowest value is not an outlier.
#p < 0.05 means to reject the null hypothesis.
grubbs.test(data_sample$soil_moisture, type = 10, opposite = FALSE)
grubbs.test(data_sample$soil_moisture, type = 10, opposite = TRUE)
#p>0.05 for both higher and lower, outliers unlikely. 

grubbs.test(target_data$soil_moisture, type = 10, opposite = FALSE)
grubbs.test(target_data$soil_moisture, type = 10, opposite = TRUE)
#p>0.05 for lower, outliers unlikely.
#p<0.05 for higher, outliers likely.
```

```{r}
#Boxplot
boxplot(data_sample$soil_moisture, target_data$soil_moisture,
        main = "Boxplot to Determine Outliers",
        names = c("sample","target"),
        xlab = "Soil Moisture")

# did not show outliers,but target data appears diminished in range.

#Histogram
hist(data_sample$soil_moisture,
     main = "Soil Moisture of data sample")
hist(target_data$soil_moisture,
     main = "Soil Moisture of target data")
#Sample is extremely skewed to the left, while target is slightly skewed to the right.
#Reason why boxplot appears diminise=hed is because sample has a much wider range of values than target.


##Quantile-quantile plot
#To compare the distribution 
qqnorm(data_sample$soil_moisture, main = "Soil Moisture of data sample")
qqline(data_sample$soil_moisture)
#significantly deviates from normal distribution

qqnorm(target_data$soil_moisture, main = "Soil Moisture of target data")
qqline(target_data$soil_moisture)
#Does not significantly deviate from normal distribution
```



```{r}
z_score_outliers(data_sample$soil_moisture, 2)
z_score_outliers(target_data$soil_moisture, 2)
```

```{r}
#Skewness & Kurtosis (in the moments library)
#If data are skewed, median may be more appropriate than mean for use
#If data has high or low kurtosis, it is more likely to have outliers

# For the purpose of this analysis, a range of -1 to +1 and -3 to +3 will be considered an acceptable range for  skewness and kurtosis respectively. Skewness values between -1 and +1 indicate that the data is roughly symmetric, while kurtosis values between -3 and +3 indicate that the data has a moderate level of peakedness and tail thickness.

skewness(data_sample$soil_moisture, na.rm = TRUE) #data is slightly skewed to the 
#left. despite the histogram, skewness is -0.738676 (<1), meaning the data is not significantly asymmetrical.
kurtosis(data_sample$soil_moisture, na.rm = TRUE) #data is mesokurtic (normal)
#kurtosis = -1.448518.
#outliers have no impact on the data.


skewness(target_data$soil_moisture, na.rm = TRUE) #data is skewed to the 
#right (value is >0 and positive) 0.4297896 is a relatively symmetrical.
kurtosis(target_data$soil_moisture, na.rm = TRUE) #data is mesokurtic (normal) 
#kurtosis = 0.4770204
#outliers do not seem to have any impact on the data.

```



## NA Handling

### Checking for null values

```{r}
sapply(data_sample, function(x)sum(is.na(x)))

sapply(target_data, function(x)sum(is.na(x)))
head(target_data) #no NA's in top row
tail(target_data) #no NA's in bottom row
```

### Interpolation
```{r}
#na.approx is part of the zoo library. It applies linear interpolation
#The 2 in apply makes it apply the function column-wise
interpolate <- function(data){
  data <- data %>% mutate(row_id = row_number())
  identifiers <- data %>% select(datetime, day, hour, coordinates, row_id)
  data <- data %>% select(-datetime, -day, -hour, -coordinates)
  interpolated <- as.data.frame(apply(data, 2, function(x)na.approx(x, na.rm = FALSE)))
  final <- sqldf("select identifiers.*, interpolated.*
                 from identifiers
                 join interpolated
                 on (identifiers.row_id = interpolated.row_id)")
  final <- final %>% select(-row_id, -row_id)
  return(final)
}
```

#### Sample

```{r}
# dim(data_sample)
# data_sample <- interpolate(data_sample)
# dim(data_sample)
# sapply(data_sample, function(x)sum(is.na(x)))
```

#### Target

```{r}
dim(target_data)
target_data <- interpolate(target_data)
dim(target_data)
sapply(target_data, function(x)sum(is.na(x)))
```



```{r}
#write.csv(target_data, "output3.csv", row.names = FALSE)
```

# Time Series Analysis

```{r}
# # time series
# # https://stats.stackexchange.com/questions/173610/arima-extract-date-time-information-from-arima-model
# # 
```



```{r}
#Creating a time series dataset for surface temperature
surf_temp_ts <- ts(target_data$surface_temperature, start = c(1,1), end = c(31,8), frequency = 8)
#Testing for stationarity
plot(surf_temp_ts)
adf.test(surf_temp_ts) #p value is > 0.05, therefore data is not stationary
```
```{r}
decompose(surf_temp_ts)
```
```{r}
# sadj <- surf_temp_ts - decomp$seasonal
```
```{r}
# plot(decompose(sadj))
# adf.test(sadj)
```
```{r}
# sadj <- diff(sadj)
# adf.test(sadj)
# tsdisplay(sadj)
```
```{r}
#Differencing 
surf_temp_ts <- diff(surf_temp_ts)
adf.test(surf_temp_ts) #Data is now stationary (p<0.05).
```
```{r}
plot(decompose(surf_temp_ts))
```

```{r}
length(surf_temp_ts) #one observation was lost to differencing
```

```{r}
##Checking the PACF and ACF to determine the best model
tsdisplay(surf_temp_ts[1:(length(surf_temp_ts)*0.8)])
```

## Arima Model

```{r}
arima_ts <- function(data, train_size,  plot=FALSE, xlab, ylab, title){

  train <- data[1:(length(data)*train_size)]
  test <- data[(length(train)+1):length(data)]
  
  arima_model <- auto.arima(train)
  print(arima_model)
  arima_forecast <- forecast(arima_model, h = length(test))
  arima_accuracy <- accuracy(arima_forecast, test)[2,2]
  
  arima_plot <- function(predicted, actual, xlab, ylab, title){
    predicted_test <- predicted$mean
    test_df <- data.frame(
      time = time(predicted_test),
      predicted = predicted_test,
      actual = actual
      )
    ggplot(test_df, aes(x = time)) +
      geom_line(aes(y = predicted, color = "Predicted")) +
      geom_line(aes(y = actual, color = "Actual")) +
      labs(x = xlab, y = ylab, title = title)
    }
  
  if (plot == TRUE){
    visual <- arima_plot(predicted = arima_forecast, actual = test, xlab=xlab, ylab=ylab, title=title)
    print(visual)
  }
  return(list(Forecast = arima_forecast$mean, RMSE = arima_accuracy, residuals = arima_forecast$residuals))
}
```

```{r}
arima_pred <- arima_ts(surf_temp_ts, train_size = 0.8, plot = TRUE, xlab ="Time", ylab = "Surface Temperature",
              title = "ARIMA Model Predicted vs Actual Temperatures")
```

```{r}
#Checking residuals
tsdisplay(arima_pred$residuals)
#The PACF and the ACF for the residuals mostly fall within the threshold, meaning that the residuals are not correlated. This means that the model has accounted for autocorrelation in the data.
#These residuals are random and show no seasonal pattern, meaning that there is no bias in the model fitting. This means that the model has also accurately accounted for seasonality in the data.
hist(arima_pred$residuals)
shapiro.test(arima_pred$residuals) #Null hypothesis is that data is normal.
#Both show that the residuals follow a normal distribution, meaning that the data is consistent with the ARIMA model's assumption of normality.
#All this suggests that the model was a good fit for this analysis.
```

# Machine Learning

## Linear Regression
```{r}
LM_reg <- function(x, y, train_size){
  data <- data.frame(x = x, y = y)
  train <- data[1:(nrow(data)*train_size),]
  test <- data[(nrow(train)+1):nrow(data),]

  model <- lm(y~x, data = train)
  LM_forecast <- predict(model, newdata = test)
  LM_accuracy <- accuracy(LM_forecast, test$y)[2]
  
  cat("RMSE:", LM_accuracy, " ")
  return(list(Forecast = LM_forecast, RMSE = LM_accuracy))
}
```


```{r}
LM_pred <- LM_reg(x = target_data$datetime, y = target_data$surface_temperature, train_size = 0.8)
```


## Random Forest
```{r}
RF_reg <- function(x, y, train_size, n_tree){
  
  data <- data.frame(x = x, y = y)
  train <- data[1:(nrow(data)*train_size),]
  test <- data[(nrow(train)+1):nrow(data),]
  
  model <- randomForest(y~x, data = train, ntree = n_tree)
  RF_forecast <- predict(model, newdata = test)
  RF_accuracy <- accuracy(RF_forecast, test$y)[2]
  
  cat("RMSE:", RF_accuracy, " ")
  return(list(Forecast = RF_forecast, RMSE = RF_accuracy))
}
```


```{r}
RF_pred_100 <- RF_reg(target_data$datetime, target_data$surface_temperature, train_size = 0.8, n_tree = 100)

RF_pred_500 <- RF_reg(target_data$datetime, target_data$surface_temperature, train_size = 0.8, n_tree = 500)

RF_pred_1000 <- RF_reg(target_data$datetime, target_data$surface_temperature, train_size = 0.8, n_tree = 1000)
```


## Support Vector Regression
```{r}
SV_reg <- function(x, y, train_size, kernel){
  
  data <- data.frame(x = x, y = y)
  train <- data[1:(nrow(data)*train_size),]
  test <- data[(nrow(train)+1):nrow(data),]
  
  model <- svm(y~x, data = train, kernel = kernel)
  SV_forecast <- predict(model, newdata = test)
  SV_accuracy <- accuracy(SV_forecast, test$y)[2]
  
  cat("RMSE:", SV_accuracy, " ")
  return(list(Forecast=SV_forecast, RMSE=SV_accuracy))
}
```


```{r}
SV_pred_linear <- SV_reg(target_data$datetime, target_data$surface_temperature, train_size = 0.8, kernel = "linear")

SV_pred_poly <- SV_reg(target_data$datetime, target_data$surface_temperature, train_size = 0.8, kernel = "polynomial")

SV_pred_radial <- SV_reg(target_data$datetime, target_data$surface_temperature, train_size = 0.8, kernel = "radial")
```



## Comparing Accuracy of the models
```{r}
#Just draw a table or something, idk.
```

```{r}
# Create a dataframe of model error values

accuracy_compare <- data.frame(
  Models = c("ARIMA", "Linear Regression", "SVR Radial", "SVR Linear", "SVR Poly", "RF 100 Trees", "RF 500 Trees", "RF 1000 Trees"),
  RMSE = c(arima_pred$RMSE, LM_pred$RMSE, SV_pred_radial$RMSE, SV_pred_linear$RMSE, SV_pred_poly$RMSE,
          RF_pred_100$RMSE, RF_pred_500$RMSE, RF_pred_1000$RMSE)
) %>% 
  mutate(RMSE = round(RMSE,3))

min_rmse <- min(accuracy_compare$RMSE)
best_model <- accuracy_compare$Models[which.min(accuracy_compare$RMSE)]

view(accuracy_compare)
```
```{r}
ggplot(accuracy_compare, aes(x = reorder(Models, RMSE), y = RMSE, fill = reorder(Models, RMSE))) +
  geom_bar(stat = "identity") +
  xlab("Model") +
  ylab("RMSE") +
  ggtitle("Comparison of RMSE for Different Models") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_text(aes(label = RMSE), vjust = -0.3) +
  annotate("text", x = 4.5, y = -0.35, label = paste0("Best Model = ", best_model, ". RMSE= ", min_rmse))
```

# Future Work/Multivariate analysis
```{r}
corrplot(cor(target_data %>% select(-datetime, -coordinates)))
```

```{r}
#Selecting variables with correlation to surface temperature
multivariate_data <- target_data %>% select(datetime, day, hour, surface_temperature, 
                                     humidity_2m, soil_temperature, soil_moisture)


#Future Work can include multivariate analysis using these features.

```


```{r}
# new_columns <- as.character(unlist(wrfdata[1, 1:ncol(wrfdata)])) #getting names of first row
# 
# new_columns <- gsub('"', "", new_columns) #removing quotes from elements
# 
# #Checking unique column names in new_columns and the old dataset
# unique(new_columns)
# unique(as.character(wrfdata[1, 1:ncol(wrfdata)]))
# 
# which(is.na(new_columns)) #checking indices
# sum(is.na(new_columns)) #checking total number
# 
# u_names <- unique(new_columns) #assigning unique values to a vector
# which(is.na(u_names)) #checking the index of NA
# u_names <- u_names[c(-13, -1, -2)] #dropping longitude, latitude, and NA
# u_names <- rep(u_names, length.out = (ncol(wrfdata)-2)) #repeating the remaining
# #values to match the number of columns in the dataset (minus XLONG and XLAT)
# 
# unique(u_names)
# 
# 
# length(u_names) #confirming that it is the correct length
# 
# my_seq <- ave(u_names, u_names, FUN = seq_along) #ave takes the first argument
# #as a numeric factor, the second element as a grouping factor, and FUN= as the
# #function to use. seq_along creates a sequence of numbers representing how many
# #times an element of the grouping factor occurs. Here, the grouping factor and
# #numerical factor are the same (u_names). If you only put "u_names" once, it
# #will not group the names. so you'll have tsk1-tsk2480 instead of tsk1-8 for each.
# 
# # paste(u_names, ave(u_names, u_names, FUN = seq_along), sep="_")
# 
# # Use the sequence number to create the new variables
# u_names <- paste(u_names, my_seq, sep = ".") #paste prints x and y with the
# #separator between them.
# 
# #Example:
# # a <- c(1,2,5)
# # b <- c("q,r,s")
# # paste(a, b, sep=" ")
# 
# 
# 
# u_names <- append(u_names, "XLAT", after = 0) #putting XLAT at the 
# #beginning of the list
# u_names <- append(u_names, "XLONG", after = 1)
# 
# length(u_names)
# 
# newdata <- wrfdata
# colnames(newdata) <- u_names
# newdata <- newdata[-1,] #deletes the first row (where the names were)
# 
# sum(is.na(newdata$XLAT))
# sum(is.na(newdata$XLONG))
# 
# #Some columns have null longitude and latitude columns, so they'll be removed.
# newdata <- newdata %>% 
#   filter(complete.cases(XLAT) & complete.cases(XLONG))
# 
# newdata$coordinates <- paste(newdata$XLAT, ",", newdata$XLONG) 
# 
# which(names(newdata) == "coordinates") #finding the index of coordinates (2483)
# ncol(newdata) #finding the total number of columns
# 
# newdata <- newdata %>% 
#   select(-2483, everything()) %>%
#   select(1:2, 2483, 3:2482)
# 
# head(newdata, n = 5)
# 
# sum(is.na(newdata$XLAT))
# sum(is.na(newdata$XLONG))
# newdata$coordinates
# 
# length(unique(newdata$coordinates)) == length(newdata$coordinates) #There are no duplicate coordinate values
# 
# 
# 
# # write.csv(newdata, "output1.csv", row.names = FALSE)
```

```{r}
# output2 <- read.csv("output2.csv")
# head(output2)

# final_data <- newdata[1,] %>%
#  select(-(1:2)) %>%
#  gather(key = "variable", value = "value") %>%
#  separate(col = "variable", into = c("variable", "time"), sep = "\\.") %>%
#  mutate(time = as.numeric(time)) %>%
#  arrange(time) %>%
#  spread(key = "variable", value = "value") %>%
#  cbind(newdata[1, 1:2], .)
# view(final_data)
# 
# class(final_data$time)
```

```{r}
# df2 <- data[1:200,]
# df2 <- df2 %>% mutate(row_id = row_number())
# 
# 
# id <- df2 %>% select(c(XLAT, XLONG, row_id))
# 
# 
# df3 <- df2 %>%
#   select(-c(XLAT, XLONG)) %>%
#   gather(key = "variable", value = "value", -row_id) %>%
#   separate(col = "variable", into = c("variable", "time"), sep = "\\-") %>%
#   mutate(time = as.numeric(time)) %>%
#   arrange(time) %>%
#   arrange(row_id, time) %>% # sort by row ID and time
#   spread(key = "variable", value = "value")
# 
# #unique(stacked$row_id)
# 
# final <- sqldf("SELECT a.XLAT, a.XLONG, b.* FROM identifiers a JOIN stacked b ON a.row_id = b.row_id")
# final <- final %>% select(-row_id)#, -time)
# 
# final <- as.data.frame(lapply(final, as.numeric))
# 
# #summary(final)
# 
# # 
# # length(unique(newdata[1:200,]$XLAT))
# # view(final)
# # write.csv(final, "data_final.csv", row.names = FALSE)
```

```{r}
# data <- newdata[1:10, ]
# data <- data %>% mutate(row_id = row_number())
# 
# identifiers <- data %>% select(c(XLAT, XLONG, coordinates, row_id))
# 
# stacked <- data %>%
#   select(-c(XLAT, XLONG, coordinates)) %>%
#   gather(key = "variable", value = "value", -row_id) %>%
#   separate(col = "variable", into = c("variable", "SN"), sep = "\\.") %>%
#   mutate(SN = as.numeric(SN)) %>%
#   arrange(SN) %>%
#   arrange(row_id, SN) %>% # sort by row ID and SN
#   spread(key = "variable", value = "value")
# 
# identified <- sqldf("SELECT a.XLAT, a.XLONG, a.coordinates, b.* FROM identifiers a JOIN stacked b ON a.row_id = b.row_id")
# identified <- identified %>% select(-row_id)
# final <- as.data.frame(lapply(identified %>%
#                                 select(-coordinates), as.numeric)) %>%
#   mutate(coordinates = identified$coordinates) %>%
#   select(c(-coordinates, everything())) %>%
#   select(1,2,coordinates,3:ncol(identified))

```

```{r}
# getbb("Bolton, Greater Manchester")
# getbb(53.576, -2.429)
# library(osmdata)
# 
# # Define the longitude and latitude coordinates
# min_lon <- as.numeric(min(newdata$XLONG))
# min_lat <-  as.numeric(min(newdata$XLAT))
# max_lon <- as.numeric(max(newdata$XLONG))
# max_lat <- as.numeric(max(newdata$XLAT))
# 
# # Create a bounding box using the coordinates
# osm <- opq(bbox = c(min_lon, min_lat, max_lon, max_lat)) %>%
#   add_osm_feature(key = "place", value = "city") %>%
#   osmdata_sf()
# 
# 
# pcntg <- (sum(is.na(osm$osm_points$name))/length(osm$osm_points$name)) *100
# 
# 
# # Extract the city names
# osm$osm_points


# # Print the city names
# print(city_names)
# 
# names(osm$osm_points)

# bb <- opq(bbox = c(min(locations$XLONG), min(locations$XLAT), max(locations$XLONG), max(locations$XLAT)))
# 
# # fetch the OSM data within the bounding box
# osm <- osmdata_sf(bb)
# 
# # extract the names of the locations based on their latitude and longitude
# locations <- osm$osm_points[grepl("place", osm$osm_points$place), ]
# names <- locations[grepl("name", locations$other_tags), ]
# 
# # print the names of the locations
# print(names$other_tags)
```

```{r}

# add_locations <- function(data){
#   reversed_data <- data %>% 
#   reverse_geocode(lat = XLAT, lon = XLONG, 
#                   address = "address", 
#                   method = "osm",
#                   custom_query  = list("accept-language"="en-US"))
#   return(reversed_data)
# }
# 
# reverseddata <- add_locations(newdata[3500:3508,])
# 
# 
# view(reverseddata)
# 
# 
# 
# reversed_data <- newdata %>% 
#   reverse_geocode(lat = XLAT, lon = XLONG, 
#                   address = "address", 
#                   method = "osm",
#                   custom_query  = list("accept-language"="en-US"))%>%
#   select(XLAT, XLONG, coordinates, address)
#  
# 
# 
# 
# # # view resulting dataframe with address information
# 

```

```{r}
# # which(newdata$XLAT >= 59.4 & newdata$XLAT<= 59.5)# &
# # which(newdata$XLONG >=8.4)# & newdata$XLONG <= 8.5)
# # max(newdata$XLAT)
# # 
# # Normandy Latitude: 49.219 to 48.55, Longitude: -0.105 to -1.799
# #Normandy 48.8799° N, 0.1713° E (according to Google)
# newdata %>%
#   filter(XLONG >= -0.105 & XLONG <= -1.799)%>%
#   filter(XLAT >= 48.55 & XLAT <= 49.219)


# which(newdata$XLONG >= -0.105 & newdata$XLONG <= -1.799 & newdata$XLAT >= 48.55 & newdata$XLONG <= 49.219)
# max_long <- 5.096
# min_long <- -0.011
# min_lat <- 48.871
# max_lat <- 59.829




# newdata[3500:3510,]
# #Carantilly is in Normandy, it's row 3500
# 
# newdata[3750,]
# #Bedford according to google maps, huntingdonshire according to openstreetmap
# #but it looks like Bedford is in huntingdonshire
# 
# #Use one of them. I'm too tired to give you advice on which one.
# newdata[3650,]
# #Normandy according to osm, Les Loges according to google. Les Loges is located within Normandy.
# #Les Loges is a farming village, apparently. Capture from row 3400 to 3750 for EDA
```

```{r}
# add_date(reorder_data(newdata[1,]), dates) #Test
# as.numeric(add_date(reorder_data(newdata[1,]), dates)[1,6:15]) == as.numeric(newdata[1, 4:13])
```

```{r}
# data <- data_sample
# IQR_value <- IQR(data$surface_temperature, na.rm = TRUE)
# q1 <- quantile(data$surface_temperature, 0.25, na.rm = TRUE)
# q3 <- quantile(data$surface_temperature, 0.75, na.rm = TRUE)
# lower_bound <- q1 - 1.5 * IQR_value
# upper_bound <- q3 + 1.5 * IQR_value
# 
# outliers1 <- data$surface_temperature[data$surface_temperature < lower_bound | data$surface_temperature > upper_bound]
# length(outliers1)
# data <- data %>% mutate(surface_temperature = Winsorize(surface_temperature,
#                                                    probs = c(0.05, 0.95), na.rm = TRUE))
# 
# outliers2 <- data$surface_temperature[data$surface_temperature < lower_bound | data$surface_temperature > upper_bound]
# length(outliers2)
```
